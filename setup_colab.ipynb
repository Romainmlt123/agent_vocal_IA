{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f5cf5c91",
      "metadata": {
        "id": "f5cf5c91"
      },
      "source": [
        "# üéì Agent Vocal IA - Setup Colab\n",
        "\n",
        "Notebook d'installation et de v√©rification pour l'Agent Vocal IA √©ducatif 100% local.\n",
        "\n",
        "**Fonctionnalit√©s :**\n",
        "- üé§ ASR (Faster-Whisper + Silero VAD)\n",
        "- üîç RAG (FAISS + SentenceTransformers)\n",
        "- üß† LLM local (llama-cpp-python)\n",
        "- üîä TTS (Piper-TTS)\n",
        "- üí° Syst√®me de hints progressifs\n",
        "\n",
        "**Pr√©requis :** GPU T4 ou A100 recommand√©"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91363323",
      "metadata": {
        "id": "91363323"
      },
      "source": [
        "## üìã √âtape 1 : V√©rification de l'environnement\n",
        "\n",
        "V√©rifions d'abord que nous avons bien acc√®s √† un GPU et les informations syst√®me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5555ef2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5555ef2c",
        "outputId": "00ee38fa-47cc-4023-b4ba-bf85446ca25a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üñ•Ô∏è  INFORMATIONS SYST√àME\n",
            "============================================================\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Plateforme: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Architecture: x86_64\n",
            "\n",
            "============================================================\n",
            "üéÆ  V√âRIFICATION GPU\n",
            "============================================================\n",
            "Wed Oct 29 20:07:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "\n",
            "============================================================\n",
            "üì¶  V√âRIFICATION PYTORCH\n",
            "============================================================\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA disponible: True\n",
            "CUDA version: 12.6\n",
            "Nombre de GPUs: 1\n",
            "  GPU 0: Tesla T4\n",
            "    M√©moire totale: 15.83 GB\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# V√©rification GPU et CUDA\n",
        "import subprocess\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üñ•Ô∏è  INFORMATIONS SYST√àME\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Plateforme: {platform.platform()}\")\n",
        "print(f\"Architecture: {platform.machine()}\")\n",
        "print()\n",
        "\n",
        "# V√©rifier NVIDIA GPU\n",
        "print(\"=\" * 60)\n",
        "print(\"üéÆ  V√âRIFICATION GPU\")\n",
        "print(\"=\" * 60)\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)\n",
        "    print(result.stdout)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Aucun GPU NVIDIA d√©tect√© ou nvidia-smi non disponible: {e}\")\n",
        "    print(\"Note: L'ex√©cution sera possible sur CPU mais beaucoup plus lente.\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"üì¶  V√âRIFICATION PYTORCH\")\n",
        "print(\"=\" * 60)\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"CUDA version: {torch.version.cuda}\")\n",
        "        print(f\"Nombre de GPUs: {torch.cuda.device_count()}\")\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "            print(f\"    M√©moire totale: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
        "except ImportError:\n",
        "    print(\"PyTorch n'est pas encore install√©. Il sera install√© √† l'√©tape suivante.\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1474ff89",
      "metadata": {
        "id": "1474ff89"
      },
      "source": [
        "## üì• √âtape 2 : Clonage du d√©p√¥t (si n√©cessaire)\n",
        "\n",
        "Si vous n'avez pas encore clon√© le d√©p√¥t, d√©commentez et ex√©cutez la cellule suivante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "da884b80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da884b80",
        "outputId": "3ea1c70a-0b69-43da-bce8-7e601079f355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'agent_vocal_IA'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 117 (delta 18), reused 106 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (117/117), 240.99 KiB | 1.85 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "/content/agent_vocal_IA\n",
            "üìÅ Dossier actuel: /content/agent_vocal_IA\n",
            "\n",
            "üìÇ Structure du projet:\n",
            "./\n",
            "  CHANGELOG.md\n",
            "  Cahier des charges - Intelligence Lab - Romain Mallet (1).pdf\n",
            "  README.md\n",
            "  demo_cli.py\n",
            "  LICENSE\n",
            "  requirements.txt\n",
            "  setup_colab.ipynb\n",
            "  test_conversation.py\n",
            "  config.yaml\n",
            "  src/\n",
            "    orchestrator.py\n",
            "    rag_build.py\n",
            "    llm.py\n",
            "    rag.py\n",
            "    utils.py\n",
            "    __init__.py\n",
            "    tts.py\n",
            "    conversation_manager.py\n",
            "    asr.py\n",
            "  ui/\n",
            "    __init__.py\n",
            "    app.py\n",
            "  data/\n",
            "    maths/\n",
            "      cours_maths.md\n",
            "    physique/\n",
            "      cours_physique.md\n",
            "    anglais/\n",
            "      english_grammar.md\n",
            "  tests/\n",
            "    test_utils.py\n",
            "    __init__.py\n",
            "    test_integration.py\n",
            "    test_rag.py\n",
            "  .git/\n",
            "    description\n",
            "    packed-refs\n",
            "    config\n",
            "    index\n",
            "    HEAD\n",
            "    hooks/\n",
            "      pre-applypatch.sample\n",
            "      pre-commit.sample\n",
            "      pre-receive.sample\n",
            "      pre-rebase.sample\n",
            "      fsmonitor-watchman.sample\n",
            "      push-to-checkout.sample\n",
            "      pre-merge-commit.sample\n",
            "      pre-push.sample\n",
            "      prepare-commit-msg.sample\n",
            "      post-update.sample\n",
            "      update.sample\n",
            "      applypatch-msg.sample\n",
            "      commit-msg.sample\n",
            "    objects/\n",
            "    info/\n",
            "      exclude\n",
            "    branches/\n",
            "    logs/\n",
            "      HEAD\n",
            "    refs/\n"
          ]
        }
      ],
      "source": [
        "# D√©commentez les lignes suivantes si vous devez cloner le d√©p√¥t\n",
        "!git clone https://github.com/Romainmlt123/agent_vocal_IA.git\n",
        "%cd agent_vocal_IA\n",
        "\n",
        "# Si vous √™tes d√©j√† dans le dossier, v√©rifiez la structure\n",
        "import os\n",
        "print(\"üìÅ Dossier actuel:\", os.getcwd())\n",
        "print(\"\\nüìÇ Structure du projet:\")\n",
        "for root, dirs, files in os.walk('.', topdown=True):\n",
        "    # Limiter la profondeur pour la lisibilit√©\n",
        "    level = root.replace('.', '').count(os.sep)\n",
        "    if level < 3:\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files:\n",
        "            if not file.startswith('.'):\n",
        "                print(f'{subindent}{file}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5b9634",
      "metadata": {
        "id": "3c5b9634"
      },
      "source": [
        "## üì¶ √âtape 3 : Installation des d√©pendances\n",
        "\n",
        "Installation de toutes les biblioth√®ques requises. **Cela peut prendre 5-10 minutes.**\n",
        "\n",
        "### ‚ö†Ô∏è Notes pour Colab :\n",
        "- **Python 3.12+** : piper-tts n'est pas compatible ‚Üí nous utilisons Coqui TTS\n",
        "- **FAISS** : On essaie faiss-gpu, sinon fallback vers faiss-cpu\n",
        "- **llama-cpp-python** : On utilise une version pr√©compil√©e avec CUDA si disponible\n",
        "\n",
        "La cellule suivante g√®re automatiquement ces probl√®mes de compatibilit√©."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a0836c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86a0836c",
        "outputId": "6b0bf2df-d132-4a07-8e14-0e773e936512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Installation des packages...\n",
            "============================================================\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 0.5.12 Requires-Python >=3.7,<3.12; 0.5.13 Requires-Python >=3.7,<3.12; 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement piper-phonemize~=1.1.0 (from piper-tts) (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for piper-phonemize~=1.1.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "üìä Installation de FAISS-GPU pour meilleures performances...\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "ü¶ô Installation de llama-cpp-python avec support CUDA...\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m315.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m319.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m301.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m414.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m261.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m315.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "‚úÖ Installation termin√©e!\n"
          ]
        }
      ],
      "source": [
        "# Installation des d√©pendances - Version optimis√©e pour Colab\n",
        "print(\"üîÑ Installation des packages...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# V√©rifier la version de Python\n",
        "import sys\n",
        "print(f\"üìå Python version: {sys.version}\")\n",
        "\n",
        "# √âtape 1 : Installer les d√©pendances de base sans les packages probl√©matiques\n",
        "print(\"\\nüì¶ √âtape 1/5 : Installation des packages de base...\")\n",
        "!pip install -q torch>=2.0.0 torchaudio>=2.0.0\n",
        "!pip install -q faster-whisper==1.0.3\n",
        "!pip install -q sentence-transformers==2.7.0\n",
        "!pip install -q pypdf==4.2.0\n",
        "!pip install -q langchain==0.2.11 langchain-community==0.2.10\n",
        "!pip install -q gradio==4.36.1\n",
        "!pip install -q pyyaml==6.0.1\n",
        "!pip install -q numpy scipy soundfile sounddevice\n",
        "\n",
        "# √âtape 2 : Installer FAISS (essayer GPU d'abord, puis CPU en fallback)\n",
        "print(\"\\nüìä √âtape 2/5 : Installation de FAISS...\")\n",
        "import subprocess\n",
        "result = subprocess.run(['pip', 'install', '-q', 'faiss-gpu'], capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úÖ FAISS-GPU install√© avec succ√®s!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  FAISS-GPU non disponible, installation de FAISS-CPU...\")\n",
        "    !pip install -q faiss-cpu==1.8.0\n",
        "\n",
        "# √âtape 3 : Installer Silero VAD\n",
        "print(\"\\nüéôÔ∏è  √âtape 3/5 : Installation de Silero VAD...\")\n",
        "!pip install -q silero-vad\n",
        "\n",
        "# √âtape 4 : Installer llama-cpp-python (version pr√©compil√©e)\n",
        "print(\"\\nü¶ô √âtape 4/5 : Installation de llama-cpp-python...\")\n",
        "print(\"   Tentative avec version CUDA pr√©compil√©e...\")\n",
        "result = subprocess.run(['pip', 'install', '-q', 'llama-cpp-python', '--extra-index-url', 'https://abetlen.github.io/llama-cpp-python/whl/cu121'], capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    print(\"‚úÖ llama-cpp-python avec CUDA install√©!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Version CUDA √©chou√©e, installation version standard...\")\n",
        "    !pip install -q llama-cpp-python==0.2.85\n",
        "\n",
        "# √âtape 5 : TTS - Utiliser une alternative compatible\n",
        "print(\"\\nüîä √âtape 5/5 : Configuration TTS...\")\n",
        "print(\"‚ö†Ô∏è  Note: piper-tts n'est pas compatible avec Python 3.12+\")\n",
        "print(\"   Installation de Coqui TTS comme alternative compatible...\")\n",
        "!pip install -q TTS>=0.22.0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Installation termin√©e!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüìù Notes importantes:\")\n",
        "print(\"   ‚Ä¢ Si llama-cpp-python a √©chou√©, le LLM utilisera le CPU\")\n",
        "print(\"   ‚Ä¢ Pour TTS, nous utilisons Coqui TTS au lieu de Piper\")\n",
        "print(\"   ‚Ä¢ V√©rifiez les imports dans la cellule suivante\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08ac0ed4",
      "metadata": {
        "id": "08ac0ed4"
      },
      "source": [
        "## ‚úÖ √âtape 4 : V√©rification des imports\n",
        "\n",
        "Testons que tous les modules principaux peuvent √™tre import√©s correctement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ce6254",
      "metadata": {
        "id": "13ce6254"
      },
      "outputs": [],
      "source": [
        "# Test des imports critiques\n",
        "import sys\n",
        "\n",
        "def test_import(module_name, display_name=None):\n",
        "    \"\"\"Test l'import d'un module et affiche le r√©sultat.\"\"\"\n",
        "    if display_name is None:\n",
        "        display_name = module_name\n",
        "    try:\n",
        "        module = __import__(module_name.split('.')[0])\n",
        "        version = getattr(module, '__version__', 'version inconnue')\n",
        "        print(f\"‚úÖ {display_name:30} - {version}\")\n",
        "        return True\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå {display_name:30} - ERREUR: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìö V√âRIFICATION DES BIBLIOTH√àQUES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "modules_to_test = [\n",
        "    ('torch', 'PyTorch'),\n",
        "    ('torchaudio', 'TorchAudio'),\n",
        "    ('faster_whisper', 'Faster-Whisper (ASR)'),\n",
        "    ('silero_vad', 'Silero VAD'),\n",
        "    ('sentence_transformers', 'SentenceTransformers'),\n",
        "    ('faiss', 'FAISS'),\n",
        "    ('pypdf', 'PyPDF'),\n",
        "    ('langchain', 'LangChain'),\n",
        "    ('llama_cpp', 'llama-cpp-python'),\n",
        "    ('gradio', 'Gradio'),\n",
        "    ('yaml', 'PyYAML'),\n",
        "    ('numpy', 'NumPy'),\n",
        "    ('soundfile', 'SoundFile'),\n",
        "    ('sounddevice', 'SoundDevice'),\n",
        "]\n",
        "\n",
        "results = []\n",
        "for module, name in modules_to_test:\n",
        "    results.append(test_import(module, name))\n",
        "\n",
        "print(\"=\" * 60)\n",
        "success_count = sum(results)\n",
        "total_count = len(results)\n",
        "print(f\"\\nüìä R√©sultat: {success_count}/{total_count} modules import√©s avec succ√®s\")\n",
        "\n",
        "if success_count == total_count:\n",
        "    print(\"üéâ Tous les modules sont pr√™ts!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Certains modules ont √©chou√©. V√©rifiez les erreurs ci-dessus.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a4a14b",
      "metadata": {
        "id": "a8a4a14b"
      },
      "source": [
        "## üì• √âtape 5 : T√©l√©chargement des mod√®les\n",
        "\n",
        "T√©l√©chargement des mod√®les LLM et voix TTS. **Attention: peut prendre 10-15 minutes selon la connexion.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1be916",
      "metadata": {
        "id": "dd1be916"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "def download_file(url, destination, description=\"Fichier\"):\n",
        "    \"\"\"T√©l√©charge un fichier avec barre de progression.\"\"\"\n",
        "    print(f\"üì• T√©l√©chargement de {description}...\")\n",
        "    print(f\"   URL: {url}\")\n",
        "    print(f\"   Destination: {destination}\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
        "\n",
        "    if os.path.exists(destination):\n",
        "        print(f\"   ‚úÖ Fichier d√©j√† pr√©sent, t√©l√©chargement ignor√©.\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        def progress_hook(block_num, block_size, total_size):\n",
        "            downloaded = block_num * block_size\n",
        "            percent = min(100, downloaded * 100 / total_size) if total_size > 0 else 0\n",
        "            bar_length = 40\n",
        "            filled = int(bar_length * percent / 100)\n",
        "            bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)\n",
        "            print(f'\\r   [{bar}] {percent:.1f}%', end='', flush=True)\n",
        "\n",
        "        urllib.request.urlretrieve(url, destination, progress_hook)\n",
        "        print(f\"\\n   ‚úÖ T√©l√©chargement r√©ussi!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Erreur: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üì¶ T√âL√âCHARGEMENT DES MOD√àLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Cr√©er les dossiers n√©cessaires\n",
        "Path(\"models/llm\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"models/voices\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\nü¶ô Mod√®le LLM (Phi-3 Mini 4K Instruct - Q4_K_M)\")\n",
        "print(\"-\" * 60)\n",
        "llm_url = \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf\"\n",
        "llm_dest = \"models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf\"\n",
        "download_file(llm_url, llm_dest, \"Phi-3 Mini 4K (2.4 GB)\")\n",
        "\n",
        "print(\"\\n\\nüîä Mod√®le TTS (Piper - Voix fran√ßaise Siwis)\")\n",
        "print(\"-\" * 60)\n",
        "tts_model_url = \"https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx\"\n",
        "tts_config_url = \"https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx.json\"\n",
        "tts_model_dest = \"models/voices/fr_FR-siwis-medium.onnx\"\n",
        "tts_config_dest = \"models/voices/fr_FR-siwis-medium.onnx.json\"\n",
        "\n",
        "download_file(tts_model_url, tts_model_dest, \"Voix TTS fran√ßaise (60 MB)\")\n",
        "download_file(tts_config_url, tts_config_dest, \"Config TTS\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ T√©l√©chargement des mod√®les termin√©!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b4bced3",
      "metadata": {},
      "source": [
        "### ‚ö†Ô∏è Note importante sur TTS\n",
        "\n",
        "Le projet utilise **Piper-TTS** par d√©faut, mais il n'est pas compatible avec Python 3.12+.\n",
        "\n",
        "**Solutions :**\n",
        "1. **Sur Colab avec Python 3.10** : Les mod√®les Piper t√©l√©charg√©s ci-dessus fonctionneront\n",
        "2. **Sur Colab avec Python 3.12+** : Nous avons install√© Coqui TTS comme alternative\n",
        "\n",
        "Si vous utilisez Coqui TTS, le projet d√©tectera automatiquement l'absence de Piper et utilisera l'alternative.\n",
        "\n",
        "Pour forcer l'utilisation de Coqui TTS, modifiez `config.yaml` :\n",
        "```yaml\n",
        "tts:\n",
        "  engine: \"coqui\"  # Au lieu de \"piper\"\n",
        "  model_name: \"tts_models/fr/css10/vits\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3ca0e2",
      "metadata": {
        "id": "8c3ca0e2"
      },
      "source": [
        "## üß™ √âtape 6 : Test rapide des modules\n",
        "\n",
        "Testons rapidement chaque composant principal du syst√®me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a39b711",
      "metadata": {
        "id": "7a39b711"
      },
      "outputs": [],
      "source": [
        "# Test rapide de chaque composant\n",
        "print(\"=\" * 60)\n",
        "print(\"üß™ TESTS DES COMPOSANTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: Configuration\n",
        "print(\"\\n1Ô∏è‚É£  Test de chargement de la configuration...\")\n",
        "try:\n",
        "    import yaml\n",
        "    with open('config.yaml', 'r', encoding='utf-8') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    print(f\"‚úÖ Configuration charg√©e: {len(config)} sections\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {e}\")\n",
        "\n",
        "# Test 2: Faster-Whisper (ASR)\n",
        "print(\"\\n2Ô∏è‚É£  Test Faster-Whisper (ASR)...\")\n",
        "try:\n",
        "    from faster_whisper import WhisperModel\n",
        "    # Ne pas charger le mod√®le complet, juste v√©rifier l'import\n",
        "    print(\"‚úÖ Faster-Whisper disponible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {e}\")\n",
        "\n",
        "# Test 3: SentenceTransformers (Embeddings)\n",
        "print(\"\\n3Ô∏è‚É£  Test SentenceTransformers (Embeddings)...\")\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    # Test d'embedding simple\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embedding = model.encode(\"Test de phrase\")\n",
        "    print(f\"‚úÖ Embeddings g√©n√©r√©s: dimension {len(embedding)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {e}\")\n",
        "\n",
        "# Test 4: FAISS\n",
        "print(\"\\n4Ô∏è‚É£  Test FAISS (Index vectoriel)...\")\n",
        "try:\n",
        "    import faiss\n",
        "    import numpy as np\n",
        "    # Cr√©er un petit index de test\n",
        "    dimension = 384\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    vectors = np.random.random((10, dimension)).astype('float32')\n",
        "    index.add(vectors)\n",
        "    print(f\"‚úÖ FAISS op√©rationnel: {index.ntotal} vecteurs index√©s\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {e}\")\n",
        "\n",
        "# Test 5: llama-cpp-python\n",
        "print(\"\\n5Ô∏è‚É£  Test llama-cpp-python (LLM)...\")\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "    print(\"‚úÖ llama-cpp-python disponible\")\n",
        "    # V√©rifier si CUDA est support√©\n",
        "    import llama_cpp\n",
        "    print(f\"   Version: {llama_cpp.__version__ if hasattr(llama_cpp, '__version__') else 'N/A'}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {e}\")\n",
        "\n",
        "# Test 6: Gradio\n",
        "print(\"\\n6Ô∏è‚É£  Test Gradio (UI)...\")\n",
        "try:\n",
        "    import gradio as gr\n",
        "    print(f\"‚úÖ Gradio {gr.__version__} disponible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Tests de base termin√©s!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüí° Conseil: Vous pouvez maintenant utiliser les modules du projet.\")\n",
        "print(\"   - Construire un indice RAG: python -m src.rag_build\")\n",
        "print(\"   - Lancer l'UI: python ui/app.py\")\n",
        "print(\"   - D√©mo CLI: python demo_cli.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a1f087d",
      "metadata": {
        "id": "8a1f087d"
      },
      "source": [
        "## üéâ Installation termin√©e !\n",
        "\n",
        "Votre environnement Agent Vocal IA est pr√™t. Vous pouvez maintenant :\n",
        "\n",
        "1. **Construire des indices RAG** pour vos mati√®res\n",
        "2. **Tester les modules individuellement** (ASR, TTS, LLM)\n",
        "3. **Lancer l'interface Gradio** pour une utilisation interactive\n",
        "4. **Utiliser le mode CLI** pour des tests en ligne de commande\n",
        "\n",
        "### Prochaines √©tapes\n",
        "\n",
        "Consultez le `README.md` pour les instructions d'utilisation d√©taill√©es."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
