{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cf5c91",
   "metadata": {},
   "source": [
    "# üéì Agent Vocal IA - Setup Colab\n",
    "\n",
    "Notebook d'installation et de v√©rification pour l'Agent Vocal IA √©ducatif 100% local.\n",
    "\n",
    "**Fonctionnalit√©s :**\n",
    "- üé§ ASR (Faster-Whisper + Silero VAD)\n",
    "- üîç RAG (FAISS + SentenceTransformers)\n",
    "- üß† LLM local (llama-cpp-python)\n",
    "- üîä TTS (Piper-TTS)\n",
    "- üí° Syst√®me de hints progressifs\n",
    "\n",
    "**Pr√©requis :** GPU T4 ou A100 recommand√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91363323",
   "metadata": {},
   "source": [
    "## üìã √âtape 1 : V√©rification de l'environnement\n",
    "\n",
    "V√©rifions d'abord que nous avons bien acc√®s √† un GPU et les informations syst√®me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5555ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification GPU et CUDA\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üñ•Ô∏è  INFORMATIONS SYST√àME\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Plateforme: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "print()\n",
    "\n",
    "# V√©rifier NVIDIA GPU\n",
    "print(\"=\" * 60)\n",
    "print(\"üéÆ  V√âRIFICATION GPU\")\n",
    "print(\"=\" * 60)\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Aucun GPU NVIDIA d√©tect√© ou nvidia-smi non disponible: {e}\")\n",
    "    print(\"Note: L'ex√©cution sera possible sur CPU mais beaucoup plus lente.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"üì¶  V√âRIFICATION PYTORCH\")\n",
    "print(\"=\" * 60)\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"Nombre de GPUs: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"    M√©moire totale: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch n'est pas encore install√©. Il sera install√© √† l'√©tape suivante.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474ff89",
   "metadata": {},
   "source": [
    "## üì• √âtape 2 : Clonage du d√©p√¥t (si n√©cessaire)\n",
    "\n",
    "Si vous n'avez pas encore clon√© le d√©p√¥t, d√©commentez et ex√©cutez la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da884b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©commentez les lignes suivantes si vous devez cloner le d√©p√¥t\n",
    "# !git clone https://github.com/Romainmlt123/agent_vocal_IA.git\n",
    "# %cd agent_vocal_IA\n",
    "\n",
    "# Si vous √™tes d√©j√† dans le dossier, v√©rifiez la structure\n",
    "import os\n",
    "print(\"üìÅ Dossier actuel:\", os.getcwd())\n",
    "print(\"\\nüìÇ Structure du projet:\")\n",
    "for root, dirs, files in os.walk('.', topdown=True):\n",
    "    # Limiter la profondeur pour la lisibilit√©\n",
    "    level = root.replace('.', '').count(os.sep)\n",
    "    if level < 3:\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            if not file.startswith('.'):\n",
    "                print(f'{subindent}{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b9634",
   "metadata": {},
   "source": [
    "## üì¶ √âtape 3 : Installation des d√©pendances\n",
    "\n",
    "Installation de toutes les biblioth√®ques requises. **Cela peut prendre 5-10 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances depuis requirements.txt\n",
    "print(\"üîÑ Installation des packages...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Installer les d√©pendances principales\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Pour Colab, installer faiss-gpu au lieu de faiss-cpu pour de meilleures performances\n",
    "print(\"\\nüìä Installation de FAISS-GPU pour meilleures performances...\")\n",
    "!pip install -q faiss-gpu\n",
    "\n",
    "# V√©rifier llama-cpp-python avec support CUDA\n",
    "print(\"\\nü¶ô Installation de llama-cpp-python avec support CUDA...\")\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -q llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n",
    "\n",
    "print(\"\\n‚úÖ Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac0ed4",
   "metadata": {},
   "source": [
    "## ‚úÖ √âtape 4 : V√©rification des imports\n",
    "\n",
    "Testons que tous les modules principaux peuvent √™tre import√©s correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test des imports critiques\n",
    "import sys\n",
    "\n",
    "def test_import(module_name, display_name=None):\n",
    "    \"\"\"Test l'import d'un module et affiche le r√©sultat.\"\"\"\n",
    "    if display_name is None:\n",
    "        display_name = module_name\n",
    "    try:\n",
    "        module = __import__(module_name.split('.')[0])\n",
    "        version = getattr(module, '__version__', 'version inconnue')\n",
    "        print(f\"‚úÖ {display_name:30} - {version}\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {display_name:30} - ERREUR: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìö V√âRIFICATION DES BIBLIOTH√àQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "modules_to_test = [\n",
    "    ('torch', 'PyTorch'),\n",
    "    ('torchaudio', 'TorchAudio'),\n",
    "    ('faster_whisper', 'Faster-Whisper (ASR)'),\n",
    "    ('silero_vad', 'Silero VAD'),\n",
    "    ('sentence_transformers', 'SentenceTransformers'),\n",
    "    ('faiss', 'FAISS'),\n",
    "    ('pypdf', 'PyPDF'),\n",
    "    ('langchain', 'LangChain'),\n",
    "    ('llama_cpp', 'llama-cpp-python'),\n",
    "    ('gradio', 'Gradio'),\n",
    "    ('yaml', 'PyYAML'),\n",
    "    ('numpy', 'NumPy'),\n",
    "    ('soundfile', 'SoundFile'),\n",
    "    ('sounddevice', 'SoundDevice'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for module, name in modules_to_test:\n",
    "    results.append(test_import(module, name))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "success_count = sum(results)\n",
    "total_count = len(results)\n",
    "print(f\"\\nüìä R√©sultat: {success_count}/{total_count} modules import√©s avec succ√®s\")\n",
    "\n",
    "if success_count == total_count:\n",
    "    print(\"üéâ Tous les modules sont pr√™ts!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Certains modules ont √©chou√©. V√©rifiez les erreurs ci-dessus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4a14b",
   "metadata": {},
   "source": [
    "## üì• √âtape 5 : T√©l√©chargement des mod√®les\n",
    "\n",
    "T√©l√©chargement des mod√®les LLM et voix TTS. **Attention: peut prendre 10-15 minutes selon la connexion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1be916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "def download_file(url, destination, description=\"Fichier\"):\n",
    "    \"\"\"T√©l√©charge un fichier avec barre de progression.\"\"\"\n",
    "    print(f\"üì• T√©l√©chargement de {description}...\")\n",
    "    print(f\"   URL: {url}\")\n",
    "    print(f\"   Destination: {destination}\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(destination):\n",
    "        print(f\"   ‚úÖ Fichier d√©j√† pr√©sent, t√©l√©chargement ignor√©.\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        def progress_hook(block_num, block_size, total_size):\n",
    "            downloaded = block_num * block_size\n",
    "            percent = min(100, downloaded * 100 / total_size) if total_size > 0 else 0\n",
    "            bar_length = 40\n",
    "            filled = int(bar_length * percent / 100)\n",
    "            bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)\n",
    "            print(f'\\r   [{bar}] {percent:.1f}%', end='', flush=True)\n",
    "        \n",
    "        urllib.request.urlretrieve(url, destination, progress_hook)\n",
    "        print(f\"\\n   ‚úÖ T√©l√©chargement r√©ussi!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n   ‚ùå Erreur: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì¶ T√âL√âCHARGEMENT DES MOD√àLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cr√©er les dossiers n√©cessaires\n",
    "Path(\"models/llm\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"models/voices\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nü¶ô Mod√®le LLM (Phi-3 Mini 4K Instruct - Q4_K_M)\")\n",
    "print(\"-\" * 60)\n",
    "llm_url = \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "llm_dest = \"models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf\"\n",
    "download_file(llm_url, llm_dest, \"Phi-3 Mini 4K (2.4 GB)\")\n",
    "\n",
    "print(\"\\n\\nüîä Mod√®le TTS (Piper - Voix fran√ßaise Siwis)\")\n",
    "print(\"-\" * 60)\n",
    "tts_model_url = \"https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx\"\n",
    "tts_config_url = \"https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx.json\"\n",
    "tts_model_dest = \"models/voices/fr_FR-siwis-medium.onnx\"\n",
    "tts_config_dest = \"models/voices/fr_FR-siwis-medium.onnx.json\"\n",
    "\n",
    "download_file(tts_model_url, tts_model_dest, \"Voix TTS fran√ßaise (60 MB)\")\n",
    "download_file(tts_config_url, tts_config_dest, \"Config TTS\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ T√©l√©chargement des mod√®les termin√©!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ca0e2",
   "metadata": {},
   "source": [
    "## üß™ √âtape 6 : Test rapide des modules\n",
    "\n",
    "Testons rapidement chaque composant principal du syst√®me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rapide de chaque composant\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ TESTS DES COMPOSANTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Configuration\n",
    "print(\"\\n1Ô∏è‚É£  Test de chargement de la configuration...\")\n",
    "try:\n",
    "    import yaml\n",
    "    with open('config.yaml', 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"‚úÖ Configuration charg√©e: {len(config)} sections\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# Test 2: Faster-Whisper (ASR)\n",
    "print(\"\\n2Ô∏è‚É£  Test Faster-Whisper (ASR)...\")\n",
    "try:\n",
    "    from faster_whisper import WhisperModel\n",
    "    # Ne pas charger le mod√®le complet, juste v√©rifier l'import\n",
    "    print(\"‚úÖ Faster-Whisper disponible\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# Test 3: SentenceTransformers (Embeddings)\n",
    "print(\"\\n3Ô∏è‚É£  Test SentenceTransformers (Embeddings)...\")\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # Test d'embedding simple\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embedding = model.encode(\"Test de phrase\")\n",
    "    print(f\"‚úÖ Embeddings g√©n√©r√©s: dimension {len(embedding)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# Test 4: FAISS\n",
    "print(\"\\n4Ô∏è‚É£  Test FAISS (Index vectoriel)...\")\n",
    "try:\n",
    "    import faiss\n",
    "    import numpy as np\n",
    "    # Cr√©er un petit index de test\n",
    "    dimension = 384\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    vectors = np.random.random((10, dimension)).astype('float32')\n",
    "    index.add(vectors)\n",
    "    print(f\"‚úÖ FAISS op√©rationnel: {index.ntotal} vecteurs index√©s\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# Test 5: llama-cpp-python\n",
    "print(\"\\n5Ô∏è‚É£  Test llama-cpp-python (LLM)...\")\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "    print(\"‚úÖ llama-cpp-python disponible\")\n",
    "    # V√©rifier si CUDA est support√©\n",
    "    import llama_cpp\n",
    "    print(f\"   Version: {llama_cpp.__version__ if hasattr(llama_cpp, '__version__') else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "# Test 6: Gradio\n",
    "print(\"\\n6Ô∏è‚É£  Test Gradio (UI)...\")\n",
    "try:\n",
    "    import gradio as gr\n",
    "    print(f\"‚úÖ Gradio {gr.__version__} disponible\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Tests de base termin√©s!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Conseil: Vous pouvez maintenant utiliser les modules du projet.\")\n",
    "print(\"   - Construire un indice RAG: python -m src.rag_build\")\n",
    "print(\"   - Lancer l'UI: python ui/app.py\")\n",
    "print(\"   - D√©mo CLI: python demo_cli.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1f087d",
   "metadata": {},
   "source": [
    "## üéâ Installation termin√©e !\n",
    "\n",
    "Votre environnement Agent Vocal IA est pr√™t. Vous pouvez maintenant :\n",
    "\n",
    "1. **Construire des indices RAG** pour vos mati√®res\n",
    "2. **Tester les modules individuellement** (ASR, TTS, LLM)\n",
    "3. **Lancer l'interface Gradio** pour une utilisation interactive\n",
    "4. **Utiliser le mode CLI** pour des tests en ligne de commande\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "Consultez le `README.md` pour les instructions d'utilisation d√©taill√©es."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
