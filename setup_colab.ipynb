{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎓 Agent Vocal IA - Setup Colab\n",
    "\n",
    "**⚠️ IMPORTANT** : Activez le GPU → Runtime → Change runtime type → T4 GPU → Save\n",
    "\n",
    "Installation optimisée pour Python 3.12+ avec gestion automatique des erreurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Étape 1 : Vérification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, platform\n",
    "print(\"=\" * 60)\n",
    "print(f\"🖥️  Python {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(\"=\" * 60)\n",
    "try:\n",
    "    subprocess.run(['nvidia-smi'], check=True, capture_output=True)\n",
    "    print(\"✅ GPU détecté!\")\n",
    "except:\n",
    "    print(\"⚠️  Aucun GPU - Activez-le: Runtime → Change runtime type → T4 GPU\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Étape 2 : Clonage du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "if os.path.exists('agent_vocal_IA'):\n",
    "    shutil.rmtree('agent_vocal_IA')\n",
    "!git clone https://github.com/Romainmlt123/agent_vocal_IA.git\n",
    "%cd agent_vocal_IA\n",
    "print(f\"✅ Projet cloné: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Étape 3 : Installation (5-10 min)\n",
    "\n",
    "**Résout automatiquement** :\n",
    "- ✅ PortAudio (sounddevice)\n",
    "- ✅ NumPy <2.0 (compatibilité)\n",
    "- ✅ FAISS-GPU → FAISS-CPU\n",
    "- ✅ llama-cpp-python CUDA\n",
    "- ✅ gTTS (Piper incompatible Python 3.12+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "print(\"🔄 Installation...\")\n",
    "\n",
    "# 1. PortAudio\n",
    "print(\"\\n1/8 PortAudio...\")\n",
    "!apt-get update -qq && apt-get install -qq -y portaudio19-dev > /dev/null 2>&1\n",
    "\n",
    "# 2. Packages base\n",
    "print(\"2/8 Packages base...\")\n",
    "!pip install -q torch torchaudio faster-whisper==1.0.3 sentence-transformers==2.7.0\n",
    "!pip install -q pypdf==4.2.0 langchain==0.2.11 langchain-community==0.2.10 gradio==4.36.1\n",
    "\n",
    "# 3. NumPy\n",
    "print(\"3/8 NumPy...\")\n",
    "!pip install -q 'numpy>=1.24,<2.0'\n",
    "\n",
    "# 4. Audio\n",
    "print(\"4/8 Audio...\")\n",
    "!pip install -q pyyaml soundfile sounddevice\n",
    "\n",
    "# 5. FAISS\n",
    "print(\"5/8 FAISS...\")\n",
    "if subprocess.run(['pip','install','-q','faiss-gpu'], capture_output=True).returncode != 0:\n",
    "    !pip install -q faiss-cpu==1.8.0\n",
    "\n",
    "# 6. Silero VAD\n",
    "print(\"6/8 Silero VAD...\")\n",
    "!pip install -q silero-vad\n",
    "\n",
    "# 7. llama-cpp-python\n",
    "print(\"7/8 llama-cpp-python...\")\n",
    "if subprocess.run(['pip','install','-q','llama-cpp-python','--extra-index-url','https://abetlen.github.io/llama-cpp-python/whl/cu121'], capture_output=True).returncode != 0:\n",
    "    !pip install -q llama-cpp-python==0.2.85\n",
    "\n",
    "# 8. TTS\n",
    "print(\"8/8 TTS...\")\n",
    "!pip install -q gTTS pyttsx3\n",
    "\n",
    "print(\"\\n✅ Installation terminée!\")\n",
    "print(\"📝 TTS: gTTS installé (Piper incompatible Python 3.12+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Étape 4 : Vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📚 Vérification...\\n\")\n",
    "modules = ['torch','faster_whisper','sentence_transformers','faiss','langchain','llama_cpp','gradio','numpy','sounddevice','gtts']\n",
    "ok = 0\n",
    "for m in modules:\n",
    "    try:\n",
    "        __import__(m.replace('-','_'))\n",
    "        print(f\"✅ {m}\")\n",
    "        ok += 1\n",
    "    except:\n",
    "        print(f\"❌ {m}\")\n",
    "print(f\"\\n📊 {ok}/{len(modules)} modules OK\")\n",
    "print(\"🎉 Prêt!\" if ok >= len(modules)-1 else \"⚠️  Réexécutez l'installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Étape 5 : Téléchargement modèle LLM (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"models/llm\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url = \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "dest = \"models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf\"\n",
    "\n",
    "if os.path.exists(dest):\n",
    "    print(\"✅ Modèle déjà téléchargé\")\n",
    "else:\n",
    "    print(\"📥 Téléchargement Phi-3 Mini (2.4 GB)...\")\n",
    "    def progress(b, bs, t):\n",
    "        p = min(100, b*bs*100/t) if t > 0 else 0\n",
    "        print(f\"\\r[{'█'*int(p/2.5)}{'░'*(40-int(p/2.5))}] {p:.1f}%\", end='')\n",
    "    urllib.request.urlretrieve(url, dest, progress)\n",
    "    print(\"\\n✅ Téléchargé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Étape 6 : Construction indices RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Construction des indices RAG...\\n\")\n",
    "!python -m src.rag_build --subject maths\n",
    "!python -m src.rag_build --subject physique\n",
    "!python -m src.rag_build --subject anglais\n",
    "print(\"\\n✅ Indices construits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Étape 7 : Lancement interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ui.app import launch_ui\n",
    "print(\"🚀 Lancement...\\n\")\n",
    "launch_ui(share=True)\n",
    "print(\"\\n📝 Cliquez sur le lien ci-dessus\")\n",
    "print(\"⚠️  Utilisez le mode TEXTE (TTS pas disponible sur Python 3.12+)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.12.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
