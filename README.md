# ğŸ“ Agent Vocal IA - Tuteur Ã‰ducatif Local

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Romainmlt123/agent_vocal_IA/blob/main/setup_colab.ipynb)
[![Documentation](https://img.shields.io/badge/docs-START_HERE-blue)](START_HERE.md)
[![Guide Colab](https://img.shields.io/badge/guide-COLAB_QUICKSTART-green)](COLAB_QUICKSTART.md)

Un assistant vocal Ã©ducatif **100% local** conÃ§u pour fonctionner sur Google Colab (GPU T4/A100), sans aucune dÃ©pendance Ã  des APIs externes.

**ğŸ¤ Parlez Ã  votre IA - Elle vous rÃ©pond vocalement en franÃ§ais !**

---

## ğŸš€ DÃ©marrage Ultra-Rapide

**Nouveau sur le projet ?** â†’ Consultez [START_HERE.md](START_HERE.md) (2 minutes)

**Premier lancement sur Colab ?** â†’ Suivez [COLAB_QUICKSTART.md](COLAB_QUICKSTART.md) (15 minutes)

**Documentation complÃ¨te ?** â†’ Continuez ci-dessous â¬‡ï¸

---

## ğŸ¯ Objectif

CrÃ©er un tuteur vocal IA capable de :

- ğŸ¤ **Ã‰couter** : L'utilisateur **parle** (franÃ§ais ou anglais) â†’ Transcription vocale avec Faster-Whisper + Silero VAD
- ğŸ” **Chercher** : RÃ©cupÃ©ration d'informations pertinentes via RAG (FAISS + SentenceTransformers)
- ğŸ§  **Raisonner** : GÃ©nÃ©ration de rÃ©ponses pÃ©dagogiques avec LLM local (llama-cpp-python)
- ğŸ”Š **Parler** : L'IA **rÃ©pond vocalement** â†’ SynthÃ¨se vocale avec Piper-TTS
- ğŸ’¡ **Guider** : Fournir 3 niveaux d'indices progressifs sans donner la solution complÃ¨te

### ğŸ™ï¸ Fonctionnement Vocal - Pipeline Complet

**L'utilisateur discute avec l'IA via sa voix :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ¤ CONVERSATION VOCALE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ğŸ—£ï¸  VOUS PARLEZ (FR/EN)
   â†“
   "Comment rÃ©soudre xÂ² - 4 = 0 ?"
   â†“
   
2. ğŸ‘‚ ASR (Faster-Whisper + Silero VAD)
   â†“
   Transcription : "Comment rÃ©soudre xÂ² - 4 = 0 ?"
   â†“
   
3. ğŸ” RAG (FAISS + SentenceTransformers)
   â†“
   Cherche dans data/maths/ â†’ Trouve "diffÃ©rence de carrÃ©s"
   â†“
   
4. ğŸ§  LLM Local (Phi-3 Mini avec prompt Ã©ducatif)
   â†“
   GÃ©nÃ¨re 3 indices progressifs (SANS la solution complÃ¨te)
   â†“
   
5. ğŸ”Š TTS (Piper-TTS voix franÃ§aise)
   â†“
   SynthÃ¨se vocale de la rÃ©ponse
   â†“
   
6. ğŸ‘‚ VOUS Ã‰COUTEZ la rÃ©ponse vocale
   â†“
   
7. ğŸ” Vous posez une question de suivi...
```

**âš¡ Temps total : ~15-20 secondes sur GPU T4, ~5-7 secondes sur GPU A100**

**ğŸŒ Conversation naturelle et fluide en franÃ§ais et anglais !**

### MatiÃ¨res supportÃ©es
- ğŸ“ **MathÃ©matiques** : AlgÃ¨bre, gÃ©omÃ©trie, calcul
- âš¡ **Physique** : MÃ©canique, Ã©lectricitÃ©, Ã©nergie
- ğŸ‡¬ğŸ‡§ **Anglais** : Grammaire, vocabulaire, traduction

---

## ğŸ—ï¸ Architecture

```
agent_vocal_IA/
â”œâ”€â”€ README.md                 # Documentation principale
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ config.yaml              # Configuration centralisÃ©e
â”œâ”€â”€ setup_colab.ipynb        # Installation et vÃ©rification sur Colab
â”œâ”€â”€ demo_cli.py              # DÃ©monstration CLI end-to-end
â”‚
â”œâ”€â”€ src/                     # Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ asr.py              # Reconnaissance vocale (ASR)
â”‚   â”œâ”€â”€ rag.py              # Recherche RAG
â”‚   â”œâ”€â”€ rag_build.py        # Construction des indices RAG
â”‚   â”œâ”€â”€ llm.py              # GÃ©nÃ©ration LLM locale
â”‚   â”œâ”€â”€ tts.py              # SynthÃ¨se vocale (TTS)
â”‚   â”œâ”€â”€ orchestrator.py     # Orchestration du pipeline
â”‚   â””â”€â”€ utils.py            # Utilitaires et helpers
â”‚
â”œâ”€â”€ ui/                      # Interface utilisateur
â”‚   â””â”€â”€ app.py              # Interface Gradio
â”‚
â”œâ”€â”€ data/                    # DonnÃ©es d'entraÃ®nement
â”‚   â”œâ”€â”€ maths/              # Documents de mathÃ©matiques
â”‚   â”œâ”€â”€ physique/           # Documents de physique
â”‚   â”œâ”€â”€ anglais/            # Documents d'anglais
â”‚   â””â”€â”€ indices/            # Indices FAISS gÃ©nÃ©rÃ©s
â”‚
â”œâ”€â”€ models/                  # ModÃ¨les locaux
â”‚   â”œâ”€â”€ llm/                # ModÃ¨les LLM (GGUF)
â”‚   â””â”€â”€ voices/             # Voix TTS (ONNX)
â”‚
â”œâ”€â”€ outputs/                 # Sorties gÃ©nÃ©rÃ©es
â”‚   â””â”€â”€ audio/              # Fichiers audio TTS
â”‚
â””â”€â”€ tests/                   # Tests unitaires et intÃ©gration
    â”œâ”€â”€ test_asr.py
    â”œâ”€â”€ test_rag.py
    â”œâ”€â”€ test_llm.py
    â”œâ”€â”€ test_tts.py
    â””â”€â”€ test_integration.py
```

---

## ğŸš€ Installation sur Google Colab - Guide Complet

### ğŸ“– Comment ouvrir le notebook sur Google Colab ?

**Option 1 : Depuis GitHub (RecommandÃ©)**
1. Allez sur [Google Colab](https://colab.research.google.com/)
2. Cliquez sur "Fichier" â†’ "Ouvrir un notebook"
3. SÃ©lectionnez l'onglet "GitHub"
4. Collez l'URL : `https://github.com/Romainmlt123/agent_vocal_IA`
5. SÃ©lectionnez `setup_colab.ipynb`
6. Le notebook s'ouvre automatiquement dans Colab

**Option 2 : Lien direct**
Cliquez sur ce lien : [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Romainmlt123/agent_vocal_IA/blob/main/setup_colab.ipynb)

### âš¡ Installation ComplÃ¨te - Commandes Ã  ExÃ©cuter

**Copiez et collez ces commandes dans des cellules Google Colab :**

#### ğŸ“¦ Cellule 1 : Cloner le projet et vÃ©rifier le GPU

```python
# VÃ©rifier le GPU disponible
!nvidia-smi

# Cloner le dÃ©pÃ´t
!git clone https://github.com/Romainmlt123/agent_vocal_IA.git
%cd agent_vocal_IA

# Afficher la structure
!ls -la
```

#### ğŸ“š Cellule 2 : Installer les dÃ©pendances

```python
# Installation des packages Python (prend 5-10 minutes)
!pip install -q -r requirements.txt

# FAISS GPU pour recherche vectorielle rapide
!pip install -q faiss-gpu

# llama-cpp-python avec support CUDA pour LLM local
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install -q llama-cpp-python --upgrade --force-reinstall --no-cache-dir

print("âœ… Installation terminÃ©e!")
```

#### ğŸ¤– Cellule 3 : TÃ©lÃ©charger les modÃ¨les IA

```python
import os

# CrÃ©er les dossiers si nÃ©cessaire
!mkdir -p models/llm models/voices

# TÃ©lÃ©charger le modÃ¨le LLM Phi-3 Mini (2.4 GB - optimisÃ© pour T4)
print("ğŸ“¥ TÃ©lÃ©chargement du modÃ¨le LLM (2.4 GB)...")
!wget -q --show-progress -P models/llm/ https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf

# TÃ©lÃ©charger la voix TTS franÃ§aise (60 MB)
print("ğŸ”Š TÃ©lÃ©chargement de la voix franÃ§aise...")
!wget -q --show-progress -P models/voices/ https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx
!wget -q --show-progress -P models/voices/ https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx.json

# VÃ©rifier les tÃ©lÃ©chargements
print("\nâœ… ModÃ¨les tÃ©lÃ©chargÃ©s:")
!ls -lh models/llm/
!ls -lh models/voices/
```

#### ğŸ” Cellule 4 : Construire les indices RAG (base de connaissances)

```python
# Construire les indices FAISS pour les 3 matiÃ¨res
# Ces indices permettent la recherche rapide dans les documents

print("ğŸ”¨ Construction des indices RAG...")
!python -m src.rag_build --subject maths
!python -m src.rag_build --subject physique
!python -m src.rag_build --subject anglais

print("\nâœ… Indices RAG crÃ©Ã©s:")
!ls -lh data/indices/
```

#### âœ… Cellule 5 : VÃ©rifier l'installation

```python
from src.utils import check_environment, setup_logging
import logging

setup_logging(level='INFO')

print("ğŸ” VÃ©rification de l'environnement...\n")
env = check_environment()

for component, available in env.items():
    status = "âœ…" if available else "âŒ"
    print(f"{status} {component}")

if all(env.values()):
    print("\nğŸ‰ Tout est prÃªt! Vous pouvez lancer l'interface.")
else:
    print("\nâš ï¸ Certains composants manquent. RevÃ©rifiez l'installation.")
```

#### ğŸ¤ Cellule 6 : Lancer l'interface vocale interactive

```python
# Lancer l'interface Gradio avec partage public
from ui.app import launch_ui

print("ğŸš€ Lancement de l'interface Agent Vocal IA...")
print("ğŸ“± L'interface sera accessible via un lien public")
print("\nâš ï¸ IMPORTANT:")
print("   1. Cliquez sur le lien qui s'affichera")
print("   2. Autorisez l'accÃ¨s au microphone dans votre navigateur")
print("   3. Cliquez sur ğŸ¤ pour enregistrer votre question")
print("   4. Parlez en franÃ§ais ou en anglais")
print("   5. L'IA vous rÃ©pondra avec des indices progressifs\n")

# Lancement avec partage public (gÃ©nÃ¨re un lien temporaire)
launch_ui(share=True)
```

---

## ğŸ¤ Comment Utiliser l'Agent Vocal ?

### Mode d'Interaction Principal : **VOCAL**

L'Agent Vocal IA est conÃ§u pour une **conversation vocale naturelle** :

1. **ğŸ¤ Activez votre micro** : Cliquez sur le bouton microphone dans l'interface
2. **ğŸ’¬ Posez votre question** : Parlez naturellement en franÃ§ais ou en anglais
   - "Comment rÃ©soudre une Ã©quation du second degrÃ© ?"
   - "Explique-moi la loi d'Ohm"
   - "How do you form the present perfect tense?"
3. **ğŸ”„ Le systÃ¨me traite votre voix** :
   - **ASR** : Transcrit votre voix en texte (Faster-Whisper)
   - **RAG** : Cherche dans les documents pertinents (FAISS)
   - **LLM** : GÃ©nÃ¨re une rÃ©ponse pÃ©dagogique avec indices progressifs
   - **TTS** : Convertit la rÃ©ponse en voix (Piper-TTS)
4. **ğŸ”Š Ã‰coutez la rÃ©ponse** : L'IA vous rÃ©pond vocalement avec 3 niveaux d'indices
5. **ğŸ” Continuez la conversation** : Posez des questions de suivi

### ğŸŒ Langues SupportÃ©es

- **ğŸ‡«ğŸ‡· FranÃ§ais** : ASR (reconnaissance), LLM (comprÃ©hension), TTS (voix franÃ§aise naturelle)
- **ğŸ‡¬ğŸ‡§ Anglais** : ASR (reconnaissance), LLM (comprÃ©hension), TTS (voix franÃ§aise pour rÃ©ponses)

**Note** : L'IA comprend et rÃ©pond en franÃ§ais ET en anglais, mais la voix de sortie est en franÃ§ais (voix Piper franÃ§aise).

### ğŸ’¡ SystÃ¨me d'Indices Progressifs

L'IA ne donne **JAMAIS** la solution complÃ¨te directement. Elle fournit 3 niveaux d'aide :

1. **Indice Niveau 1 (LÃ©ger)** : Question guidante ou rappel de concept
2. **Indice Niveau 2 (Moyen)** : MÃ©thode ou approche Ã  utiliser  
3. **Indice Niveau 3 (Fort)** : DÃ©but de rÃ©solution mais sans la rÃ©ponse finale

**Exemple :**
```
Vous : "Comment rÃ©soudre xÂ² + 2x - 3 = 0 ?"

IA : "
  Indice 1 : As-tu pensÃ© Ã  la formule du discriminant ?
  Indice 2 : Calcule d'abord Î” = bÂ² - 4ac avec a=1, b=2, c=-3
  Indice 3 : Tu trouves Î” = 16. Maintenant utilise les formules xâ‚ et xâ‚‚
"
```

---

## ğŸ’» Utilisation AvancÃ©e

### ğŸ¯ Mode Interface Vocale (RecommandÃ©)

**Dans Google Colab :**

```python
from ui.app import launch_ui

# Lance l'interface avec lien public
launch_ui(share=True)
```

Cliquez sur le lien gÃ©nÃ©rÃ© (ex: `https://xxxxx.gradio.live`) pour accÃ©der Ã  l'interface.

**Ã‰tapes d'utilisation :**
1. Autorisez l'accÃ¨s au microphone dans votre navigateur
2. SÃ©lectionnez la matiÃ¨re (MathÃ©matiques, Physique, Anglais)
3. Cliquez sur ğŸ¤ et posez votre question vocalement
4. Attendez la transcription et la rÃ©ponse vocale
5. Continuez la conversation

### ğŸ“ Mode CLI (Ligne de commande - Pour tests)

```bash
# Mode interactif avec dÃ©tection automatique de matiÃ¨re
python demo_cli.py --interactive

# Test avec un fichier audio
python demo_cli.py --audio input.wav --subject maths

# Test avec texte
python demo_cli.py --text "Explique-moi le thÃ©orÃ¨me de Pythagore"
```

### ğŸ”§ Commandes de maintenance

```bash
# Reconstruire un indice RAG
python -m src.rag_build --subject maths --input data/maths/

# Tester ASR seul
python -m src.asr --demo --audio test.wav

# Tester TTS seul
python -m src.tts --text "Bonjour, je suis votre tuteur vocal" --output output.wav
```

---

## ğŸ§ª Tests

ExÃ©cuter tous les tests :

```bash
pytest tests/ -v
```

Tests individuels :

```bash
# ASR
pytest tests/test_asr.py -v

# RAG
pytest tests/test_rag.py -v

# LLM
pytest tests/test_llm.py -v

# TTS
pytest tests/test_tts.py -v

# IntÃ©gration complÃ¨te
pytest tests/test_integration.py -v
```

---

## âš™ï¸ Configuration

Tous les paramÃ¨tres sont dans `config.yaml` :

- **ASR** : ModÃ¨le Whisper, langue, VAD
- **RAG** : ModÃ¨le d'embeddings, chunk size, top-k
- **LLM** : Chemin du modÃ¨le, tempÃ©rature, tokens max
- **TTS** : Voix, vitesse, sample rate
- **Orchestrator** : DÃ©tection auto de matiÃ¨re, keywords

### Exemple : Changer le modÃ¨le LLM

```yaml
llm:
  model_path: "models/llm/qwen2.5-3b-instruct.Q4_K_M.gguf"
  n_gpu_layers: 35
  temperature: 0.7
```

---

## ğŸ§  FonctionnalitÃ©s ClÃ©s

### 1. ğŸ¤ Reconnaissance Vocale (ASR)
- **ModÃ¨le** : Faster-Whisper (Small par dÃ©faut)
- **VAD** : Silero pour dÃ©tecter les silences et segmenter la parole
- **Langues** : FranÃ§ais et Anglais (dÃ©tection automatique)
- **Streaming** : Transcription en temps rÃ©el possible
- **EntrÃ©e** : Microphone direct ou fichiers audio (WAV, MP3)

### 2. ğŸ” RAG (Retrieval Augmented Generation)
- **Embeddings** : SentenceTransformers (all-MiniLM-L6-v2)
- **Index** : FAISS pour recherche vectorielle rapide sur GPU/CPU
- **Sources** : PDF et TXT, chunking intelligent avec overlap
- **MatiÃ¨res** : Base de connaissances en maths, physique, anglais
- **Top-K** : RÃ©cupÃ¨re les 3 passages les plus pertinents

### 3. ğŸ§  LLM Local (GÃ©nÃ©ration de RÃ©ponses)
- **ModÃ¨les supportÃ©s** : Phi-3, Qwen, Mistral (format GGUF)
- **InfÃ©rence** : llama-cpp-python avec GPU offloading (35 layers)
- **Langues** : Comprend franÃ§ais et anglais
- **SystÃ¨me de hints** : 3 niveaux d'indices progressifs sans donner la solution
- **Contexte** : Utilise les documents RAG pour des rÃ©ponses prÃ©cises

### 4. ğŸ”Š SynthÃ¨se Vocale (TTS)
- **Moteur** : Piper-TTS (neural TTS)
- **Voix** : FranÃ§ais natif (siwis medium - naturelle et claire)
- **QualitÃ©** : 22kHz, streaming pour textes longs
- **Sortie** : Fichiers WAV ou lecture directe

### 5. ğŸ¨ Interface Gradio (UI Vocale)
- âœ… Enregistrement audio en un clic depuis le navigateur
- âœ… Autorisation microphone automatique
- âœ… SÃ©lection de matiÃ¨re (ou dÃ©tection auto)
- âœ… Affichage en temps rÃ©el : transcription â†’ rÃ©ponse â†’ sources RAG
- âœ… Lecture audio de la rÃ©ponse
- âœ… Historique de conversation
- âœ… Design responsive et intuitif

### 6. ğŸ¯ Orchestration Intelligente
- **Pipeline automatique** : ASR â†’ RAG â†’ LLM â†’ TTS
- **DÃ©tection de matiÃ¨re** : Keywords-based (automatique ou manuel)
- **Gestion d'erreurs** : Fallback gracieux si un composant Ã©choue
- **Historique** : MÃ©morisation du contexte conversationnel

---

## ğŸ“Š Performances

### Configuration recommandÃ©e (Colab)

| Composant | GPU T4 | GPU A100 |
|-----------|--------|----------|
| ASR (Whisper Small) | ~2s / 10s audio | ~1s / 10s audio |
| RAG (recherche) | <0.5s | <0.3s |
| LLM (gÃ©nÃ©ration 200 tokens) | ~10-15s | ~3-5s |
| TTS (1 phrase) | ~1s | ~0.5s |

### MÃ©moire

- **GPU T4 (15 GB)** : Whisper Small + Phi-3 Mini (4K) ou Qwen 2.5B
- **GPU A100 (40 GB)** : Whisper Medium + Mistral 7B ou Qwen 7B

---

## ğŸ› ï¸ DÃ©veloppement

### Standards de qualitÃ©
- âœ… Type hints sur toutes les fonctions
- âœ… Docstrings (Google style)
- âœ… Logging centralisÃ© (niveau DEBUG/INFO)
- âœ… Gestion d'erreurs explicite
- âœ… Tests unitaires et intÃ©gration
- âœ… PEP8 (formatÃ© avec Black)

### Contribuer

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit (`git commit -m 'Ajout fonctionnalitÃ© X'`)
4. Push (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

---

## ğŸ”’ Limitations et Notes Importantes

- **100% local** : Aucune API externe â†’ nÃ©cessite GPU pour performances acceptables (minimum T4)
- **Langues** : 
  - âœ… **ASR** : Comprend franÃ§ais ET anglais (dÃ©tection automatique)
  - âœ… **LLM** : RÃ©pond en franÃ§ais ET anglais selon la question
  - âš ï¸ **TTS** : Voix franÃ§aise uniquement (rÃ©pond en franÃ§ais mÃªme si vous parlez anglais)
- **MÃ©moire GPU** : 
  - **T4 (15 GB)** : Whisper Small + Phi-3 Mini (performant)
  - **A100 (40 GB)** : Whisper Medium + modÃ¨les plus gros possibles
- **PrÃ©cision RAG** : DÃ©pend de la qualitÃ© des documents fournis dans `data/`
- **Hints progressifs** : Le LLM est instruit de ne pas donner la solution complÃ¨te, mais peut parfois rÃ©vÃ©ler trop d'informations (amÃ©lioration continue via prompt engineering)
- **Latence** : 
  - Sur T4 : ~15-20 secondes par question (normal)
  - Sur A100 : ~5-7 secondes par question
- **Connexion Internet** : NÃ©cessaire uniquement pour le tÃ©lÃ©chargement initial des modÃ¨les (~3 GB). Ensuite, tout fonctionne offline.

### ğŸŒ Support Multilingue DÃ©taillÃ©

| Composant | FranÃ§ais ğŸ‡«ğŸ‡· | Anglais ğŸ‡¬ğŸ‡§ |
|-----------|--------------|-------------|
| **Ã‰coute (ASR)** | âœ… Oui | âœ… Oui |
| **ComprÃ©hension (LLM)** | âœ… Oui | âœ… Oui |
| **RÃ©ponse Ã©crite** | âœ… Oui | âœ… Oui |
| **Voix (TTS)** | âœ… Oui (voix native) | âš ï¸ Texte franÃ§ais uniquement |

**Note** : L'IA peut comprendre et transcrire l'anglais, mais rÃ©pondra toujours avec une voix franÃ§aise. Pour une voix anglaise, il faudrait tÃ©lÃ©charger une voix Piper anglaise supplÃ©mentaire.

---

## â“ FAQ - Questions FrÃ©quentes

### ğŸ¤ Comment utiliser le microphone sur Google Colab ?

1. Lancez l'interface avec `launch_ui(share=True)`
2. Cliquez sur le lien public gÃ©nÃ©rÃ© (https://xxxxx.gradio.live)
3. Autorisez l'accÃ¨s au microphone dans votre navigateur
4. Cliquez sur le bouton ğŸ¤ dans l'onglet "Mode Vocal"
5. Parlez normalement, puis arrÃªtez l'enregistrement

**Note** : Le microphone ne fonctionne PAS directement dans l'iframe Colab, utilisez toujours le lien public Gradio.

### ğŸŒ Puis-je poser des questions en anglais ?

**Oui !** L'IA comprend parfaitement l'anglais :
- âœ… Vous pouvez parler en anglais
- âœ… L'IA transcrit correctement
- âœ… L'IA rÃ©pond en anglais (texte)
- âš ï¸ Mais la voix est en franÃ§ais

Pour une voix anglaise, modifiez `config.yaml` :
```yaml
tts:
  model_path: "models/voices/en_US-amy-medium.onnx"
```
Et tÃ©lÃ©chargez une voix anglaise depuis [Piper Voices](https://huggingface.co/rhasspy/piper-voices).

### ğŸ”‡ Puis-je utiliser le systÃ¨me sans voix (texte uniquement) ?

**Oui !** Utilisez l'onglet "Mode Texte" dans l'interface Gradio, ou le CLI :

```bash
python demo_cli.py --text "Votre question ici"
```

### ğŸ“± Ã‡a marche sur mobile ?

**Oui !** Le lien Gradio public (`https://xxxxx.gradio.live`) fonctionne sur :
- ğŸ“± Smartphones (iOS/Android)
- ğŸ’» Tablettes
- ğŸ–¥ï¸ Ordinateurs

Assurez-vous d'autoriser le microphone dans votre navigateur mobile.

### â±ï¸ Pourquoi c'est lent sur T4 ?

C'est **normal** ! Le GPU T4 est gratuit mais moins puissant :
- Transcription : ~2s
- Recherche RAG : <0.5s
- GÃ©nÃ©ration LLM : ~10-15s (c'est ici que Ã§a prend du temps)
- SynthÃ¨se vocale : ~1s

**Total : ~15-20 secondes par question**

Pour accÃ©lÃ©rer :
- Utilisez un GPU A100 (si disponible)
- RÃ©duisez `llm.max_tokens` dans `config.yaml`
- Utilisez un modÃ¨le plus petit (Phi-3 Mini est dÃ©jÃ  optimisÃ©)

### ğŸ’¾ Mes donnÃ©es sont-elles sauvegardÃ©es ?

**Non**, les fichiers sur Colab sont temporaires (session de ~12h). Pour sauvegarder :

```python
# Sauvegarder sur Google Drive
from google.colab import drive
drive.mount('/content/drive')
!cp -r /content/agent_vocal_IA /content/drive/MyDrive/
```

### ğŸ“š Comment ajouter mes propres documents ?

1. Ajoutez vos fichiers PDF ou TXT dans `data/{matiere}/`
2. Reconstruisez l'indice RAG :

```python
!python -m src.rag_build --subject maths --input data/maths/
```

3. L'IA utilisera vos nouveaux documents !

### ğŸ”„ Le lien Gradio a expirÃ©, comment le relancer ?

Les liens Gradio publics expirent aprÃ¨s ~72h. Pour en gÃ©nÃ©rer un nouveau :

```python
from ui.app import launch_ui
launch_ui(share=True)
```

### ğŸ¤– Puis-je utiliser un autre modÃ¨le LLM ?

**Oui !** TÃ©lÃ©chargez un modÃ¨le GGUF depuis [Hugging Face](https://huggingface.co/models?library=gguf) :

```python
# Exemple : Mistral 7B
!wget -P models/llm/ https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
```

Puis mettez Ã  jour `config.yaml` :
```yaml
llm:
  model_path: "models/llm/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
  n_gpu_layers: 35
```

### ğŸ“ L'IA donne la solution complÃ¨te, comment l'empÃªcher ?

Le systÃ¨me utilise un **prompt Ã©ducatif** qui interdit de donner la solution. Si Ã§a arrive :

1. Ajustez la tempÃ©rature dans `config.yaml` (plus bas = plus strict) :
```yaml
llm:
  temperature: 0.5  # Au lieu de 0.7
```

2. Reformulez votre question pour demander explicitement des indices :
"Donne-moi des indices pour rÃ©soudre..." au lieu de "RÃ©sous..."

### ğŸ”´ Erreur "CUDA out of memory"

Votre GPU est saturÃ©. Solutions :

1. RedÃ©marrez le runtime Colab (Menu "ExÃ©cution" â†’ "RedÃ©marrer la session")
2. RÃ©duisez `n_gpu_layers` dans `config.yaml` :
```yaml
llm:
  n_gpu_layers: 20  # Au lieu de 35
```
3. Utilisez un modÃ¨le plus petit (Phi-3 Mini est dÃ©jÃ  optimal pour T4)

---

## ğŸ“š Ressources

### ModÃ¨les
- [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper)
- [Silero VAD](https://github.com/snakers4/silero-vad)
- [Sentence Transformers](https://www.sbert.net/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [llama.cpp Python](https://github.com/abetlen/llama-cpp-python)
- [Piper TTS](https://github.com/rhasspy/piper)

### Documentation
- [Google Colab](https://colab.research.google.com/)
- [Gradio](https://www.gradio.app/)
- [Hugging Face](https://huggingface.co/)

---

## ğŸ“„ Licence

MIT License - Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ‘¤ Auteur

**Romain Mallet** - Intelligence Lab  
ğŸ“§ Contact : [GitHub](https://github.com/Romainmlt123)

---

## ğŸ™ Remerciements

Merci aux communautÃ©s open-source qui rendent ce projet possible :
- Ã‰quipes Whisper (OpenAI), Piper (Rhasspy), FAISS (Meta)
- Contributeurs llama.cpp et llama-cpp-python
- DÃ©veloppeurs Gradio et SentenceTransformers

---

**Note** : Ce projet est conÃ§u pour l'Ã©ducation et la recherche. Les performances peuvent varier selon le matÃ©riel et les modÃ¨les utilisÃ©s.