# ğŸ“ Agent Vocal IA - Tuteur Ã‰ducatif Local

Un assistant vocal Ã©ducatif **100% local** conÃ§u pour fonctionner sur Google Colab (GPU T4/A100), sans aucune dÃ©pendance Ã  des APIs externes.

## ğŸ¯ Objectif

CrÃ©er un tuteur vocal IA capable de :

- ğŸ¤ **Ã‰couter** : Transcription vocale avec Faster-Whisper + Silero VAD
- ğŸ” **Chercher** : RÃ©cupÃ©ration d'informations pertinentes via RAG (FAISS + SentenceTransformers)
- ğŸ§  **Raisonner** : GÃ©nÃ©ration de rÃ©ponses avec LLM local (llama-cpp-python)
- ğŸ”Š **Parler** : SynthÃ¨se vocale avec Piper-TTS
- ğŸ’¡ **Guider** : Fournir 3 niveaux d'indices progressifs sans donner la solution complÃ¨te

### MatiÃ¨res supportÃ©es
- ğŸ“ **MathÃ©matiques** : AlgÃ¨bre, gÃ©omÃ©trie, calcul
- âš¡ **Physique** : MÃ©canique, Ã©lectricitÃ©, Ã©nergie
- ğŸ‡¬ğŸ‡§ **Anglais** : Grammaire, vocabulaire, traduction

---

## ğŸ—ï¸ Architecture

```
agent_vocal_IA/
â”œâ”€â”€ README.md                 # Documentation principale
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ config.yaml              # Configuration centralisÃ©e
â”œâ”€â”€ setup_colab.ipynb        # Installation et vÃ©rification sur Colab
â”œâ”€â”€ demo_cli.py              # DÃ©monstration CLI end-to-end
â”‚
â”œâ”€â”€ src/                     # Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ asr.py              # Reconnaissance vocale (ASR)
â”‚   â”œâ”€â”€ rag.py              # Recherche RAG
â”‚   â”œâ”€â”€ rag_build.py        # Construction des indices RAG
â”‚   â”œâ”€â”€ llm.py              # GÃ©nÃ©ration LLM locale
â”‚   â”œâ”€â”€ tts.py              # SynthÃ¨se vocale (TTS)
â”‚   â”œâ”€â”€ orchestrator.py     # Orchestration du pipeline
â”‚   â””â”€â”€ utils.py            # Utilitaires et helpers
â”‚
â”œâ”€â”€ ui/                      # Interface utilisateur
â”‚   â””â”€â”€ app.py              # Interface Gradio
â”‚
â”œâ”€â”€ data/                    # DonnÃ©es d'entraÃ®nement
â”‚   â”œâ”€â”€ maths/              # Documents de mathÃ©matiques
â”‚   â”œâ”€â”€ physique/           # Documents de physique
â”‚   â”œâ”€â”€ anglais/            # Documents d'anglais
â”‚   â””â”€â”€ indices/            # Indices FAISS gÃ©nÃ©rÃ©s
â”‚
â”œâ”€â”€ models/                  # ModÃ¨les locaux
â”‚   â”œâ”€â”€ llm/                # ModÃ¨les LLM (GGUF)
â”‚   â””â”€â”€ voices/             # Voix TTS (ONNX)
â”‚
â”œâ”€â”€ outputs/                 # Sorties gÃ©nÃ©rÃ©es
â”‚   â””â”€â”€ audio/              # Fichiers audio TTS
â”‚
â””â”€â”€ tests/                   # Tests unitaires et intÃ©gration
    â”œâ”€â”€ test_asr.py
    â”œâ”€â”€ test_rag.py
    â”œâ”€â”€ test_llm.py
    â”œâ”€â”€ test_tts.py
    â””â”€â”€ test_integration.py
```

---

## ğŸš€ Installation sur Google Colab

### 1. Cloner le dÃ©pÃ´t

```python
!git clone https://github.com/Romainmlt123/agent_vocal_IA.git
%cd agent_vocal_IA
```

### 2. Installer les dÃ©pendances

Ouvrez `setup_colab.ipynb` et exÃ©cutez toutes les cellules, ou utilisez :

```python
!pip install -r requirements.txt
```

### 3. VÃ©rifier l'installation

```python
# VÃ©rifier le GPU
!nvidia-smi

# Importer et tester les modules
from src.utils import check_environment
check_environment()
```

### 4. TÃ©lÃ©charger les modÃ¨les

Les modÃ¨les seront automatiquement tÃ©lÃ©chargÃ©s lors de la premiÃ¨re utilisation, ou manuellement :

```python
# LLM (exemple : Phi-3 Mini 4K Instruct Q4_K_M)
!wget -P models/llm/ https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf

# TTS (Piper French voice)
!wget -P models/voices/ https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx
!wget -P models/voices/ https://huggingface.co/rhasspy/piper-voices/resolve/main/fr/fr_FR/siwis/medium/fr_FR-siwis-medium.onnx.json
```

---

## ğŸ’» Utilisation

### Mode CLI (Ligne de commande)

```bash
# Pipeline complet : voix â†’ transcription â†’ RAG â†’ LLM â†’ TTS
python demo_cli.py --audio input.wav --subject maths

# Construire un indice RAG
python -m src.rag_build --subject maths --input data/maths/

# Test ASR seul
python -m src.asr --demo --audio test.wav

# Test TTS seul
python -m src.tts --text "Bonjour, je suis votre tuteur vocal" --output output.wav
```

### Mode UI (Interface Gradio)

```python
from ui.app import launch_ui
launch_ui()
```

Ou directement :

```bash
python ui/app.py
```

L'interface sera accessible Ã  `http://localhost:7860`

---

## ğŸ§ª Tests

ExÃ©cuter tous les tests :

```bash
pytest tests/ -v
```

Tests individuels :

```bash
# ASR
pytest tests/test_asr.py -v

# RAG
pytest tests/test_rag.py -v

# LLM
pytest tests/test_llm.py -v

# TTS
pytest tests/test_tts.py -v

# IntÃ©gration complÃ¨te
pytest tests/test_integration.py -v
```

---

## âš™ï¸ Configuration

Tous les paramÃ¨tres sont dans `config.yaml` :

- **ASR** : ModÃ¨le Whisper, langue, VAD
- **RAG** : ModÃ¨le d'embeddings, chunk size, top-k
- **LLM** : Chemin du modÃ¨le, tempÃ©rature, tokens max
- **TTS** : Voix, vitesse, sample rate
- **Orchestrator** : DÃ©tection auto de matiÃ¨re, keywords

### Exemple : Changer le modÃ¨le LLM

```yaml
llm:
  model_path: "models/llm/qwen2.5-3b-instruct.Q4_K_M.gguf"
  n_gpu_layers: 35
  temperature: 0.7
```

---

## ğŸ§  FonctionnalitÃ©s ClÃ©s

### 1. Transcription Vocale (ASR)
- **ModÃ¨le** : Faster-Whisper (Small par dÃ©faut)
- **VAD** : Silero pour dÃ©tecter les silences
- **Streaming** : Transcription en temps rÃ©el

### 2. RAG (Retrieval Augmented Generation)
- **Embeddings** : SentenceTransformers (all-MiniLM-L6-v2)
- **Index** : FAISS pour recherche vectorielle rapide
- **Sources** : PDF et TXT, chunking intelligent

### 3. LLM Local
- **ModÃ¨les supportÃ©s** : Phi-3, Qwen, Mistral (format GGUF)
- **InfÃ©rence** : llama-cpp-python avec GPU offloading
- **SystÃ¨me de hints** : 3 niveaux d'indices progressifs

### 4. SynthÃ¨se Vocale (TTS)
- **Moteur** : Piper-TTS
- **Voix** : FranÃ§ais natif (siwis, upmc, tom)
- **QualitÃ©** : 22kHz, streaming possible

### 5. Interface Gradio
- Enregistrement audio en un clic
- SÃ©lection de matiÃ¨re
- Affichage des sources RAG
- Lecture de la rÃ©ponse vocale

---

## ğŸ“Š Performances

### Configuration recommandÃ©e (Colab)

| Composant | GPU T4 | GPU A100 |
|-----------|--------|----------|
| ASR (Whisper Small) | ~2s / 10s audio | ~1s / 10s audio |
| RAG (recherche) | <0.5s | <0.3s |
| LLM (gÃ©nÃ©ration 200 tokens) | ~10-15s | ~3-5s |
| TTS (1 phrase) | ~1s | ~0.5s |

### MÃ©moire

- **GPU T4 (15 GB)** : Whisper Small + Phi-3 Mini (4K) ou Qwen 2.5B
- **GPU A100 (40 GB)** : Whisper Medium + Mistral 7B ou Qwen 7B

---

## ğŸ› ï¸ DÃ©veloppement

### Standards de qualitÃ©
- âœ… Type hints sur toutes les fonctions
- âœ… Docstrings (Google style)
- âœ… Logging centralisÃ© (niveau DEBUG/INFO)
- âœ… Gestion d'erreurs explicite
- âœ… Tests unitaires et intÃ©gration
- âœ… PEP8 (formatÃ© avec Black)

### Contribuer

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amelioration`)
3. Commit (`git commit -m 'Ajout fonctionnalitÃ© X'`)
4. Push (`git push origin feature/amelioration`)
5. Ouvrir une Pull Request

---

## ğŸ”’ Limitations

- **100% local** : Aucune API externe â†’ nÃ©cessite GPU pour performances acceptables
- **Langues** : OptimisÃ© pour le franÃ§ais (ASR/TTS), multilingue pour LLM
- **MÃ©moire** : ModÃ¨les lourds â†’ ajuster selon GPU disponible
- **PrÃ©cision RAG** : DÃ©pend de la qualitÃ© des documents fournis
- **Hints progressifs** : L'IA peut parfois rÃ©vÃ©ler trop d'informations (amÃ©lioration continue)

---

## ğŸ“š Ressources

### ModÃ¨les
- [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper)
- [Silero VAD](https://github.com/snakers4/silero-vad)
- [Sentence Transformers](https://www.sbert.net/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [llama.cpp Python](https://github.com/abetlen/llama-cpp-python)
- [Piper TTS](https://github.com/rhasspy/piper)

### Documentation
- [Google Colab](https://colab.research.google.com/)
- [Gradio](https://www.gradio.app/)
- [Hugging Face](https://huggingface.co/)

---

## ğŸ“„ Licence

MIT License - Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ‘¤ Auteur

**Romain Mallet** - Intelligence Lab  
ğŸ“§ Contact : [GitHub](https://github.com/Romainmlt123)

---

## ğŸ™ Remerciements

Merci aux communautÃ©s open-source qui rendent ce projet possible :
- Ã‰quipes Whisper (OpenAI), Piper (Rhasspy), FAISS (Meta)
- Contributeurs llama.cpp et llama-cpp-python
- DÃ©veloppeurs Gradio et SentenceTransformers

---

**Note** : Ce projet est conÃ§u pour l'Ã©ducation et la recherche. Les performances peuvent varier selon le matÃ©riel et les modÃ¨les utilisÃ©s.